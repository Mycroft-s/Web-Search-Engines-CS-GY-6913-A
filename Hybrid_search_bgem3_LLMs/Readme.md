Recent advances in information retrieval (IR) have demonstrated that combining sparse and dense retrieval methods can improve search effectiveness by leveraging both lexical and semantic cues. In parallel, Large Language Models (LLMs) have shown remarkable prowess in understanding context, generating semantically related terms, and improving downstream IR tasks through query expansion. 

This paper presents an integrated hybrid retrieval framework that employs a single model, BGE-M3 (https://huggingface.co/BAAI/bge-m3), to produce both sparse and dense vector representations for documents and queries. We store and manage these embeddings using Milvus (https://milvus.io/), a high-performance vector database, and utilize Attu (https://github.com/zilliztech/attu) for visualization and index maintenance. Our approach incorporates LLM-based query expansions to enrich user queries, followed by a hybrid retrieval strategy that fuses sparse and dense candidates. Finally, we apply a BGE-based reranking model(https://huggingface.co/BAAI/bge-reranker-large) to refine the top results and increase precision. Evaluations on a subset of MS MARCO (https://microsoft.github.io/msmarco/Submission) demonstrate that this pipeline outperforms single-mode retrieval baselines, leading to improved recall and nDCG scores. Our findings highlight the value of LLM-driven expansions and unified embedding representations in building more robust and user-friendly IR systems. 
